# Distributed File System
Creating a distributed, fault-tolerant file system in Java. When it works, it should replicate data across multiple servers and repair pieces of files that have become corrupt.

## Try it out
I've been using an old macbook running macOS Catalina to develop this code, and as such have used SDKMAN! to install the necessary packages. *sdk current* tells me I'm using Gradle 7.4.2 and Java 17.0.2-tem, both of which work for me. I haven't tested how much leniency there is with respect to how current Gradle and Java must be to run this code, but probably I'd say having a newish version of Gradle is more important than Java. 

The *Controller*, one of the three types of nodes in the program, has been written to identify the local IP address of its machine so that it may bind to it. Since the *ChunkServer* and the *Client* are designed to communicate with the *Controller*, line 17 of 'ChunkServer.java' and line 28 of 'Client.java', each located in the 'node' subdirectory of the 'src' folder, must be modified before compilation to be the same IP to that which the *Controller* binds. You can find this IP by executing *ifconfig* in a macOS terminal window. This is what my local IP looks like: 192.168.68.59. Once those two lines of code have been correctly modified, open a terminal and navigate to the highest directory of the project, which contains the 'build', 'libs', and 'src' folders. 

I've written a script called 'osx.sh' for the project, which automates the running of the project's components. To use the script, run *./osx.sh* in the terminal window. This will use Gradle to clean and build the project. Next it will start the *Controller* node in the current terminal window, and spawn two new terminal windows. In one of the new terminal windows, run *./osx.sh*. This will, in this new window, open nine terminal tabs, and create a *ChunkServer* in each of them. Wait for new terminal tabs to stop spawning before you navigate to the last terminal window. The last terminal window will be the window used for the *Client*. The *Client* can either use **erasure coding** or **replication** as a storage technique. To use erasure coding, run *./osx.sh c erasure*, to use replication, run *./osx.sh c replication*. 

If you've followed the directions correctly, and my directions are clear, you should have open three terminal windows. One will contain the *Controller*, which will be printing out data associated with the connections it has made with the *ChunkServers*. Another will contain ten terminal tabs, nine of which are each associated with a specific *ChunkServer*. Each *ChunkServer* will print its IP address and port, along with information regarding the regular heartbeats it sends to the *Controller*. The last window will contain the *Client*, which will be waiting for a command from the user. 

The *Client* offers directions on how it is to be used when you run the 'help' command (or when it is given a known command with improper usage).

The *Controller*, the *Client*, and the *ChunkServers* can all be run on different machines, but this script, for demo purposes, runs them locally on one machine. It does this by creating a folder for each *ChunkServer* in another folder called 'serverDirectories'. This way, any files stored on the distributed file system are contained within the project directory, albeit spread across folders in chunks or shards. After storing a file, it can be neat to watch the folders populate, or repair themselves after deletions.

An update: I've written 'ubuntu.sh' to work in Ubuntu 20.04.4 LTS with Gradle 4.4.1 and Java openjdk 1.8.0_312. To use in Ubuntu, just follow the above steps, replacing *./osx.sh* with *./ubuntu.sh* in all cases.

## A quick overview

This project is an assignment taken from a graduate course in distributed systems at CSU. I haven't taken the course, but I took its undergraduate prerequisite during my senior year. Since I haven't attended the lectures or the recitations, I can't guarantee that my work for this project meets the unmentioned-in-the-assignment-description-but-tested-in-their-grading requirements. This project involves a lot of code with a bunch of interacting parts, so there will be behavior that is unexpected if the right inputs are supplied.

As was briefly mentioned in the *How to try it out* section, the project is based around three types of nodes -- a node being a computer in a network executing a specific program from the project. The three types of nodes are the *Controller*, the *ChunkServer*, and the *Client*.

The *Controller* communicates with both the *Client* and the *ChunkServers*. It keeps track of the files that have been distributed over the *ChunkServers*, and at which specific server each chunk or shard resides. It is also responsible for deciding, upon receiving from the *Client* a request to store a file, which *ChunkServers* will store which portion of that file. Additionally, it receives, at regular intervals, status updates from each of the *ChunkServers* called *heartbeats*. From information contained in each heartbeat, the *Controller* must determine whether the *ChunkServer* is ripe for storage, or if it has failed. If it notices that a *ChunkServer* has failed, it will orchestrate the relocation of the lost data to another *ChunkServer*, if possible.

The *ChunkServer* is responsible for storing chunks or shards of files, sending regular heartbeat messages to the *Controller*, and serving data to the *Client*.

The *Client* communicates with the *Controller* when it wants to receive a list of the stored files, store a file, delete a file, or retrieve a file. It only interacts directly with the *ChunkServers* when it sends to them pieces of the files (chunks or shards) it wishes to store, or when it requests pieces of files for retrieval. The *Client* has more complexity built into it than one might first assume, as it must be capable of dealing with unresponsive *ChunkServers* when storing or retrieving files. When the *Client* wishes to retrieve a file, it must collect all chunks, and therefore, in the case of replication, contact a maximum of three servers per chunk (nine for erasure coding), and stitch them together to reconstruct the original file.

## Two techniques for fault tolerance

This project uses two techniques for fault tolerance, namely **erasure coding** and **replication**.

Erasure coding splits every file into **chunks**, which can vary in size, but for this project, chunks are fixed at 64KB. A chunk is further split into nine **shards**. Six of the nine shards are *data* shards, and the other three are *parity* shards. The data shards contain, as you might expect, one-sixth of the chunk's data, along with a hash of that data and other identifying information. The parity shards contain between the three of them a linear algebraic representation of the data which are capable, when combined with other data and parity shards in a specific manner, of recreating missing or corrupt data shards. *Backblaze* has provided the code used in the project for encoding, decoding, and repairing shards with Reed-Solomon erasure coding. A detailed explanation of how they work can be found [here](https://www.backblaze.com/blog/reed-solomon/).

Replication splits every file the *Client* wishes to store into 64KB chunks. This project uses a redundancy factor of three, which means that every chunk will be stored on three *ChunkServers* at any given time. If a *ChunkServer* happens to go offline, the *Controller* will realize, from the stoppage of heartbeats, that the *ChunkServer* has failed, and coordinate a relocation of that chunk to another server in order to maintain the correct replication count.

Regardless of whether the *Client* is using erasure coding or replication, every *ChunkServer* checks for missing or corrupt data before every heartbeat message is sent back to the *Controller*. The assignment calls for a *minor* heartbeat every 30 seconds and a *major* heartbeat every five minutes, but for the sake of expediency in testing, I've reduced the interval for minor and major heartbeats to 15 and 150 seconds, respectively. Minor heartbeats contain information about newly added chunks/shards while major heartbeats contain information about ALL chunks/shards. As such, every major heartbeat presents an opportunity for the *Controller* to detect inconsistencies with regard to what *it* believes should be being stored at each *ChunkServer*. If the *Controller* notices an inconsistency, it can ask another server to send a replacement of the missing or corrupt data. This is slightly more complicated when using erasure coding, as the server with the missing or corrupt data must contact up to nine other servers to reconstruct what it needs.

## Video demos

https://user-images.githubusercontent.com/32202629/166638696-445275d7-6ff2-4821-bfc9-12fd0c85bed3.mp4

In this demo, I use the 'osx.sh' script. It first cleans and builds the project, and starts the *Controller* in the open terminal window. It then spawns two new terminal windows. I use *./osx.sh* in the first to create nine *ChunksServers*. In the second, I use *./osx.sh c replication* to start the *Client* in **replication** mode (as opposed to erasure coding). I then store and delete a file called 'medium', which can be seen in the project directory in the upper right terminal. Next, I store 'medium' once again but shut down one of the *ChunkServers* responsible for storing a few of its chunks. This shutdown tells the *Controller* to find a new place for the unavailable chunks located on server eight. A replication factor of three is maintained by storing the chunks across servers seven, four, and three (one of which is eight's replacement). It can be noted that even after the final command, *delete medium*, is issued, files remain in the 'server8' directory. This is because *ChunkServer* eight has been terminated, is no longer in communication with the *Controller*, and therefore cannot delete its files.

https://user-images.githubusercontent.com/32202629/168397869-c92b4b98-2dd0-4b68-bc77-178a8076c3d8.mp4

In this demo, I again use the 'osx.sh' script. But this time, I use *./osx.sh c erasure* to start the *Client* in **erasure coding** mode, which creates nine shards for every chunk of the file. I then store, retrieve, and delete a file called 'small', which can be seen in the project directory in the upper right terminal. Next, I store a file called 'medium', but delete all of the shards on *ChunkServer* one before I attempt to retrieve it. Despite the missing shards, the retrieval is still successful, and when I navigate back to the 'server1' directory, the missing shards have been replaced. I then delete 'medium' and close the program. 

Note that the look of the statistics reported by the *Controller* has changed between the two demos. This demo is slightly more recent, and I've modified the code slightly.
