TODO:

1. When a 'ClientRequestsStoreChunk' is sent to the server, and the server hasn't stored that chunk yet, availableChunkServers doesn't automatically create 3 chunks and add them to the chunkcache for every triplet of servers that it gives out. This means that if another requests to store the same chunk comes immediately after the first, before there is a chance for the recommendations structure to be updated, we could store twice. Therefore, it should modify the data structure and add the three servers that it recommends as three chunks into recommendations. Then, if the message isn't actually sent out, the chunks can be removed. /// DONE

2. Need to send CONTROLLER_REQUESTS_FILE_FORWARD to replication servers that have a replication of a chunk held by a deregistered chunk server. If the server acknowledges the request, add the relevant file to the DistributedFileCache with the replacement chunk server's identifier. Maybe send a heartbeat to the replacement chunk server first in order to check if it is alive. May need to actually daisy chain TCP connections to actually get it done. Best effort may not work if we are trying to keep the recommendations server synchronized with who has actually received the request. /// REWORKED

3. REQUESTS_SHARD and SEND_FILE_FOR_STORAGE in TCPReceiverThread is still unfinished. It relies on some functions that need to be written in the FileDistributionService which will read shard files and check if they are corrupt. /// DONE

4. Create a message called CHUNK_SERVER_REPORTS_FILE_HEALTHY, that will be sent to the controller following a fix to a chunk server's file, which could be a result of a slice request or a file forward. // DONE

5. SEND_FILE_FOR_STORAGE will send messages to the controller about Chunk Servers that it can't connect to. It will also send messages to the controller about chunk servers that don't acknowledge the file that was sent to them. When the Controller receives a message like this, it will remove the chunk from its recommendations DistributedFileCache. It won't try to forward immediately. That will just result in more confusion, because there won't have been enough time to actually get confirmation that chunk servers have the file (they inform the controller through heartbeats). So, part of the heartbeat structure now has to include a section where files without three replications in the recommendations cache will be given a third replication server that isn't the other two, and those messages will be sent out of the heartbeat channel.

6. Finish the logic in the FileDistributionService's run method so that it can handle messages of corruption and forwarding as they come through. /// DONE/REWORKED

7. Finish the 'deregister' function in the ChunkServerConnectionCache. New homes must be found for files that the deregistering node is taking down with it. /// DONE

--never-wait-on-replies--
No thread that is responsible for dealing with received messages should send a message over the network AND block for a reply, even if there is a timeout on the blocking. This happens occasionally in the TCPReceiverThread (which itself needs to be rethought), and in the Client. The first rule of a distributed system is that you cannot tell the difference between a dead node and a slow node. Therefore, the message logic should be set up so that if a message is received, it is dealt with in a specific way, and a response is sent back out (if necessary). If sending out a response elicits another message, then THAT message will be dealt with on the next loop of the receiver thread, not the current loop. In this way, nodes can be slow or dead, and we can deal with them in other ways.

--read-write-in-blocks--
At the Client, files are read one chunk at a time, and sent to their respective ChunkServers for storage. This is inefficient (obviously). Instead, the Client should read in blocks of chunks -- twenty at a time? -- and then work through chunkizing/sharding that data and sending it off to ChunkServers. Likewise should be done when the Client is retrieving a file from the ChunkServers -- multiple chunks should be gotten and assembled in memory, and then written to disk, instead of writing to disk one chunk at a time.

--better-control-flow-nodes--
The Controller, ChunkServer, and Client main methods are a mess right now -- especially the Client -- and are in need of a re-write for readability and maintainability. To bring them more up to speed with the principles I've been using for the pastry network.

--de-monolith-tcpreceiverthread--
The TCPReceiverThread is a monster, but fret not! There are good alternatives to how it currently works! The Client should not have access to methods for receiving messages that are only to be used by the Controller, or the ChunkServer. Same for the other two node types. Changing how this works requires moving to the EventFactory model, which means adding a couple of classes to the project (which have already been written for the pastry project), and then slowly working your way through the TCPReceiverThread -- moving functions for dealing with specific message types into main node classes. Doing so will also create an opportunity to fix most of the occurences of the --never-wait-on-replies-- todo item.

--event-factory--
EventFactory isn't formatted how it should be. Need to go through message types, and consolidate certain types of messages into more generic categories for simplicity. Having a different message type/format for every single type of message that can be sent is superfluous and confusing.


LOG:

--tmux--
Created a TMUX script which, instead of spawning new terminal windows and tabs to house the running ChunkServers and the Client, does the same thing using TMUX windows. I was hoping it could be a cleaner solution to running the app locally, perhaps less vulnerable to bugs which could occur in the timing of the creation of those new windows and tabs.

--fat-jar--
The 'build.gradle' file was giving me trouble. I didn't understand how it worked, and still don't fully, but I simplified it while maintaining its functionality. The tricky part is forcing gradle to bundle the 'reed-solomon-erasure-coding.jar' with the project jar at the end so that the classes like FileDistributionService can find the ReedSolomon functions during execution.

--docker--
Created a new folder in the upmost project directory called 'docker', which contains both a 'Dockerfile' and a 'docker-compose' file. Having just taken a course covering the basics of docker, I wanted to use what I had learned to provide a way of running this app in an isolated environment. It's a complete building and running solution which uses a Gradle container to compile, and a Java container to run the necessary components. Each node type -- controller, chunkserver, and client -- has a slightly different container setup which is specified in the dockerfile. A Gradle container compiles the project once, and a copy of the compiled JAR file is given to each of the node containers to execute. Once started, the containers communicate with one another via a docker virtual network (created in the compose file), which allows containers to route to eachother via their container names. To run the app with docker, the user must first change the 'controllerHost' variable inside the 'application.properties' file to 'controller'. Then, from inside the 'docker' folder, the user must run 'docker compose build' in the command line, followed by 'docker compose up -d'. The first command builds the project, and the second command creates and starts container nodes in the background. Then, the user can use 'docker attach dfs-client-1' to attach to the running Client's I/O stream, so that they may run commands like 'put', 'get', 'ls', etc. To exit out of the I/O stream the user can use the docker escape sequence: Ctrl-p Ctrl-q. 'docker compose down' will stop all of the containers and remove them.

--properties--
After adding a docker option for running the app, it was clear that it would be too complicated to ask the user to change the values of variables within the Controller, ChunkServer, and Client classes to reflect the hostname and port where the Controller could be found. I had recently been working on another distributed systems project for the same course, and had borrowed a solution which uses Java Properties to read a file for variables that the user should be able to change. So, I copied two classes I'd completed in that project into this project, and created another folder in the upmost project directory called 'config', which contains a file called 'application.properties'. This file contains values for variables that the user can change, specifically the ones that need to be changed when running the app locally, in docker, or in a distributed environment. With this new file, the user no longer passes either 'erasure' or 'replication' to the Client as a command line argument, but chagnes the value in the 'application.properties' file.

--osx-tab--
The 'osx.sh' script was producing strange behavior. Newly created terminals/tabs default their working directory to the home directory (~/), and if a command is executed too quickly, the tab has no chance to change it to the directory of the parent terminal. Therefore, the newly created ChunkServers couldn't find the 'application.properties' file, which is found by concatenating the path from which the JAR was started with '/config/application.properties'. Worked around this issue by adding a delay between spawning the new terminal tab and executing the start command.

--refactor-nodes--
Working on refactoring the code at the Client and the Controller to be more readable, and to be more in the style of the code of the nodes in the pastry network assignment. Also have been fixing some formatting issues throughout the project -- function and variable names that go against the camelCase style that I've been using dominantly.

--brainstorming-client--
The Client doesn't need to to make use of the TCPReceiverThread that must be modified for use by Controller and the ChunkServer, so what is the best way for the Controller to communicate with the other nodes? The Client has a main thread which receives commands from the user, and although in a perfect world it would be better to keep the main thread free to take commands from the user, even while a write or a read to or from the dfs is happening, to start, I'll just write a ClientWriter and ClientReader which will be Runnable, and can perform those jobs while the main thread waits, or checks for progress on the job.



- Cleaned up eventfactory, made it a singleton

- Cleaned up TCPSender

- Starting to transfer TCPReceiverThread cases into appropriate node

- DistributedFileCache is a mess, need to improve underlying data structures of ServerFile, DistributedFile, Shard, Chunk to really come up with something easier to maintain and understand. There are too many function calls associated with getting just one piece of information, too much conversion between String, Integer, Long for there to be any comprehension of what is going on.

- Need better solution from Comparators in DistributedFileCache

- Deregistration can't be one-off best effort, must be able to keep track of state of relocations and try again if relocation fails until options are exhausted

- Need to go through all the messages and make getType() actually return the type stored in the message, not the predefined protocol. Also need to go through the messages and make them match GeneralMessage in their formatting for readability and consistency.

- Make sure all messages that can are using the GeneralMessage instead of a superfluous specific  message type. How does changing to use the GeneralMessage at the EventFactory and the Controller affect the logic at the ChunkServers and the Client? Need to think about that.

- BlockingQueue for identifiers or ConcurrentMap for ChunkServerConnectionCache? It seems like it could improve the readability of the code, with less manual locking.

- Add timestamp and version information to the Chunks and Shards. Both should be easy. Timestamp information and Version count will each be updated any time a write happens to a Chunk or a Shard. If the shard is replaced at the ChunkServer, its timestamp will be updated to the time of its writing, and its version will be incremented from wherever the Shard's data came from. Whereever the shard came from, its version will be +1 of what it was before. But, do chunks that are identical except the version number should probably be treated identically, same with the shards.

- Why is listFreestServers() returning a Vector<String> and not a Vector<Integer>? Shouldn't it be returning a list of server identifiers, which are integers?

- Instead of returning null values, some functions choose to return -1 or "" (for int and string), but why? Is there a reason that null can't convey the same thing? There seem to be different standards used throughout the code.

- What about the scenario where the Client requests to store a chunk, the Controller finds three ChunkServers that should hold that particular chunk, but one of those ChunkServers goes offline or deregisters before the Client has had a chance to store the chunk at that particular ChunkServer? How will the Client know if a storage operation is successful? Should the Client send the chunk out to the ChunkServers without wondering whether the operation was successful? Should it wait for confirmation from the ChunkServers? If it does wait for confirmation, but doesn't receive it, what should happen then? It seems like the Client should give the user some indication that the storage operation only successfully happened at N number of nodes. But if it is done that way, how long should the Client wait for confirmation before returning control of the Client to the user? Best-effort and conscientious communication with the user are at odds with one another.

- When deregistering, we shouldn't do any unnecessary deletions of the data structures that are being removed from the ChunkServerConnectionCache. That way, we won't disrupt any threads that are doing inside of the ChunkServerConnection while it is being deregistered.

- Need to create a ChunkInformation class just for storing metadata about a particular chunk/shard. Name, sequence, version, timestamp, etc. Then, that data structure can be used to replace the delimited strings that are being passed around in the ChunkServerHeartbeatService and the HeartbeatMonitor.

- More to add to deal with CHUNK_SERVER_NO_STORE_FILE case at the Controller. Need to find a new home for the missing Chunk, or is that dealt with elsewhere?
