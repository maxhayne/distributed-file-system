TODO:

1. When a 'ClientRequestsStoreChunk' is sent to the server, and the server hasn't stored that chunk yet, availableChunkServers doesn't automatically create 3 chunks and add them to the chunkcache for every triplet of servers that it gives out. This means that if another requests to store the same chunk comes immediately after the first, before there is a chance for the recommendations structure to be updated, we could store twice. Therefore, it should modify the data structure and add the three servers that it recommends as three chunks into recommendations. Then, if the message isn't actually sent out, the chunks can be removed. /// DONE

2. Need to send CONTROLLER_REQUESTS_FILE_FORWARD to replication servers that have a replication of a chunk held by a deregistered chunk server. If the server acknowledges the request, add the relevant file to the DistributedFileCache with the replacement chunk server's identifier. Maybe send a heartbeat to the replacement chunk server first in order to check if it is alive. May need to actually daisy chain TCP connections to actually get it done. Best effort may not work if we are trying to keep the recommendations server synchronized with who has actually received the request. /// REWORKED

3. REQUESTS_SHARD and SEND_FILE_FOR_STORAGE in TCPReceiverThread is still unfinished. It relies on some functions that need to be written in the FileDistributionService which will read shard files and check if they are corrupt. /// DONE

4. Create a message called CHUNK_SERVER_REPORTS_FILE_HEALTHY, that will be sent to the controller following a fix to a chunk server's file, which could be a result of a slice request or a file forward. // DONE

5. SEND_FILE_FOR_STORAGE will send messages to the controller about Chunk Servers that it can't connect to. It will also send messages to the controller about chunk servers that don't acknowledge the file that was sent to them. When the Controller receives a message like this, it will remove the chunk from its recommendations DistributedFileCache. It won't try to forward immediately. That will just result in more confusion, because there won't have been enough time to actually get confirmation that chunk servers have the file (they inform the controller through heartbeats). So, part of the heartbeat structure now has to include a section where files without three replications in the recommendations cache will be given a third replication server that isn't the other two, and those messages will be sent out of the heartbeat channel.

6. Finish the logic in the FileDistributionService's run method so that it can handle messages of corruption and forwarding as they come through. /// DONE/REWORKED

7. Finish the 'deregister' function in the ChunkServerConnectionCache. New homes must be found for files that the deregistering node is taking down with it. /// DONE

--never-wait-on-replies--
No thread that is responsible for dealing with received messages should send a message over the network AND block for a reply, even if there is a timeout on the blocking. This happens occasionally in the TCPReceiverThread (which itself needs to be rethought), and in the Client. The first rule of a distributed system is that you cannot tell the difference between a dead node and a slow node. Therefore, the message logic should be set up so that if a message is received, it is dealt with in a specific way, and a response is sent back out (if necessary). If sending out a response elicits another message, then THAT message will be dealt with on the next loop of the receiver thread, not the current loop. In this way, nodes can be slow or dead, and we can deal with them in other ways.

--read-write-in-blocks--
At the Client, files are read one chunk at a time, and sent to their respective ChunkServers for storage. This is inefficient (obviously). Instead, the Client should read in blocks of chunks -- twenty at a time? -- and then work through chunkizing/sharding that data and sending it off to ChunkServers. Likewise should be done when the Client is retrieving a file from the ChunkServers -- multiple chunks should be gotten and assembled in memory, and then written to disk, instead of writing to disk one chunk at a time.

--better-control-flow-nodes--
The Controller, ChunkServer, and Client main methods are a mess right now -- especially the Client -- and are in need of a re-write for readability and maintainability. To bring them more up to speed with the principles I've been using for the pastry network.

--de-monolith-tcpreceiverthread--
The TCPReceiverThread is a monster, but fret not! There are good alternatives to how it currently works! The Client should not have access to methods for receiving messages that are only to be used by the Controller, or the ChunkServer. Same for the other two node types. Changing how this works requires moving to the EventFactory model, which means adding a couple of classes to the project (which have already been written for the pastry project), and then slowly working your way through the TCPReceiverThread -- moving functions for dealing with specific message types into main node classes. Doing so will also create an opportunity to fix most of the occurences of the --never-wait-on-replies-- todo item.

--event-factory--
EventFactory isn't formatted how it should be. Need to go through message types, and consolidate certain types of messages into more generic categories for simplicity. Having a different message type/format for every single type of message that can be sent is superfluous and confusing.


LOG:

--tmux--
Created a TMUX script which, instead of spawning new terminal windows and tabs to house the running ChunkServers and the Client, does the same thing using TMUX windows. I was hoping it could be a cleaner solution to running the app locally, perhaps less vulnerable to bugs which could occur in the timing of the creation of those new windows and tabs.

--fat-jar--
The 'build.gradle' file was giving me trouble. I didn't understand how it worked, and still don't fully, but I simplified it while maintaining its functionality. The tricky part is forcing gradle to bundle the 'reed-solomon-erasure-coding.jar' with the project jar at the end so that the classes like FileDistributionService can find the ReedSolomon functions during execution.

--docker--
Created a new folder in the upmost project directory called 'docker', which contains both a 'Dockerfile' and a 'docker-compose' file. Having just taken a course covering the basics of docker, I wanted to use what I had learned to provide a way of running this app in an isolated environment. It's a complete building and running solution which uses a Gradle container to compile, and a Java container to run the necessary components. Each node type -- controller, chunkserver, and client -- has a slightly different container setup which is specified in the dockerfile. A Gradle container compiles the project once, and a copy of the compiled JAR file is given to each of the node containers to execute. Once started, the containers communicate with one another via a docker virtual network (created in the compose file), which allows containers to route to eachother via their container names. To run the app with docker, the user must first change the 'controllerHost' variable inside the 'application.properties' file to 'controller'. Then, from inside the 'docker' folder, the user must run 'docker compose build' in the command line, followed by 'docker compose up -d'. The first command builds the project, and the second command creates and starts container nodes in the background. Then, the user can use 'docker attach dfs-client-1' to attach to the running Client's I/O stream, so that they may run commands like 'put', 'get', 'ls', etc. To exit out of the I/O stream the user can use the docker escape sequence: Ctrl-p Ctrl-q. 'docker compose down' will stop all of the containers and remove them.

--properties--
After adding a docker option for running the app, it was clear that it would be too complicated to ask the user to change the values of variables within the Controller, ChunkServer, and Client classes to reflect the hostname and port where the Controller could be found. I had recently been working on another distributed systems project for the same course, and had borrowed a solution which uses Java Properties to read a file for variables that the user should be able to change. So, I copied two classes I'd completed in that project into this project, and created another folder in the upmost project directory called 'config', which contains a file called 'application.properties'. This file contains values for variables that the user can change, specifically the ones that need to be changed when running the app locally, in docker, or in a distributed environment. With this new file, the user no longer passes either 'erasure' or 'replication' to the Client as a command line argument, but chagnes the value in the 'application.properties' file.

--osx-tab--
The 'osx.sh' script was producing strange behavior. Newly created terminals/tabs default their working directory to the home directory (~/), and if a command is executed too quickly, the tab has no chance to change it to the directory of the parent terminal. Therefore, the newly created ChunkServers couldn't find the 'application.properties' file, which is found by concatenating the path from which the JAR was started with '/config/application.properties'. Worked around this issue by adding a delay between spawning the new terminal tab and executing the start command.

--refactor-nodes--
Working on refactoring the code at the Client and the Controller to be more readable, and to be more in the style of the code of the nodes in the pastry network assignment. Also have been fixing some formatting issues throughout the project -- function and variable names that go against the camelCase style that I've been using dominantly.

--brainstorming-client--
The Client doesn't need to to make use of the TCPReceiverThread that must be modified for use by Controller and the ChunkServer, so what is the best way for the Controller to communicate with the other nodes? The Client has a main thread which receives commands from the user, and although in a perfect world it would be better to keep the main thread free to take commands from the user, even while a write or a read to or from the dfs is happening, to start, I'll just write a ClientWriter and ClientReader which will be Runnable, and can perform those jobs while the main thread waits, or checks for progress on the job.



- Cleaned up eventfactory, made it a singleton

- Cleaned up TCPSender

- Starting to transfer TCPReceiverThread cases into appropriate node

- DistributedFileCache is a mess, need to improve underlying data structures of ServerFile, DistributedFile, Shard, Chunk to really come up with something easier to maintain and understand. There are too many function calls associated with getting just one piece of information, too much conversion between String, Integer, Long for there to be any comprehension of what is going on.

- Need better solution from Comparators in DistributedFileCache

- Deregistration can't be one-off best effort, must be able to keep track of state of relocations and try again if relocation fails until options are exhausted

- Need to go through all the messages and make getType() actually return the type stored in the message, not the predefined protocol. Also need to go through the messages and make them match GeneralMessage in their formatting for readability and consistency.

- Make sure all messages that can are using the GeneralMessage instead of a superfluous specific  message type. How does changing to use the GeneralMessage at the EventFactory and the Controller affect the logic at the ChunkServers and the Client? Need to think about that.

- BlockingQueue for identifiers or ConcurrentMap for ChunkServerConnectionCache? It seems like it could improve the readability of the code, with less manual locking.

- Add timestamp and version information to the Chunks and Shards. Both should be easy. Timestamp information and Version count will each be updated any time a write happens to a Chunk or a Shard. If the shard is replaced at the ChunkServer, its timestamp will be updated to the time of its writing, and its version will be incremented from wherever the Shard's data came from. Whereever the shard came from, its version will be +1 of what it was before. But, do chunks that are identical except the version number should probably be treated identically, same with the shards.

- Why is listFreestServers() returning a Vector<String> and not a Vector<Integer>? Shouldn't it be returning a list of server identifiers, which are integers?

- Instead of returning null values, some functions choose to return -1 or "" (for int and string), but why? Is there a reason that null can't convey the same thing? There seem to be different standards used throughout the code.

- What about the scenario where the Client requests to store a chunk, the Controller finds three ChunkServers that should hold that particular chunk, but one of those ChunkServers goes offline or deregisters before the Client has had a chance to store the chunk at that particular ChunkServer? How will the Client know if a storage operation is successful? Should the Client send the chunk out to the ChunkServers without wondering whether the operation was successful? Should it wait for confirmation from the ChunkServers? If it does wait for confirmation, but doesn't receive it, what should happen then? It seems like the Client should give the user some indication that the storage operation only successfully happened at N number of nodes. But if it is done that way, how long should the Client wait for confirmation before returning control of the Client to the user? Best-effort and conscientious communication with the user are at odds with one another.

- When deregistering, we shouldn't do any unnecessary deletions of the data structures that are being removed from the ChunkServerConnectionCache. That way, we won't disrupt any threads that are doing inside of the ChunkServerConnection while it is being deregistered.

- Need to create a ChunkInformation class just for storing metadata about a particular chunk/shard. Name, sequence, version, timestamp, etc. Then, that data structure can be used to replace the delimited strings that are being passed around in the ChunkServerHeartbeatService and the HeartbeatMonitor.

- More to add to deal with CHUNK_SERVER_NO_STORE_FILE case at the Controller. Need to find a new home for the missing Chunk, or is that dealt with elsewhere?

- Should there even be a Shard class? Since the shard is basically just a particular replication of a chunk under the erasure coding storageType?

- Need to go through wireformats and delete message types that are now just using the GeneralMessage for the same purpose. This will create errors, which will lead to locations where the GeneralMessage can be instantiated instead of the other, deleted message types.

- Have thought about this before, but should the Controller just send the Client the list of ChunkServers from the idealState or the reportedState. The problem with the reportedState is that it isn't available right away, there is a delay until the reportedState starts being populated by heartbeats. But, it does represent the actual state of the DFS assuming heartbeats are accurate. What is the best option here?

- There must be an elegant way to start the TCPServerThread and ChunkServerHeartbeatService and the FileDistributionService only once the Controller has sent confirmation to the ChunkServer of a successful registration?

- May need to come up with a better way of generating the folder that the ChunkServer will use to store files. Would it be better to create unique filenames (with host and port and id attached) than to create a folder with a unique timestamp? How could this be made cleaner, instead of being done in the registrationInterpreter?

- Maybe can start the FileDistributionService before the registration happens in the ChunkServer? It would be inactive if the ChunkServer isn't registered anyway.

- Should open connections between nodes be periodically cleared? I imagine it would become inefficient if every ChunkServer maintained an open connection to every other ChunkServer, as would happen after the DFS has been up for a while, what with file repairs and forwarding.

- Get rid of sendQueue in ChunkServerConnection, better for the Controller to be made aware of any error being thrown as it tries to send a message to a ChunkServer.

- sendGeneralMessage, sendRegistration, and sendDeregistration should all behave similarly, in that they should all either throw an exception, or shouldn't, which is best for the ChunkServer? And, should they all be added to a different class which houses functions for sending messages out of specified connections? Actually, it will be best to get rid of the sendRegistration and sendDeregistration functions, and just use the sendGeneralMessage with the correct types and messages for registration and deregistration. sendGeneralMessage should throw an exception if it fails.

- getDirectory() in FileDistributionService has changed, and now returns a Path, not a string. Therefore, any place this function has called must now be inspected to ensure that the output of getDirectory() is not concatted with a String, as was the behavior before. Instead of concatenation, new usage will be to use getDirectory().resolve(filename) to create a path object that points to the file we're trying to operate on.

- The "Files" package does support appending files, if you use additional options in the call to Files.write(), but it does not support editing a section of a file in place. That shouldn't be an issue in this scenario, as chunks are 64K and shards are around 8K -- the file can be read fully, and the byte string can be edited, and then the byte string can be rewritten -- but this will have to be implemented in the replaceSlices(), truncateFile()?, appendFile(), and getNextChunkFromFile()?

- Decide whether to work with Paths consistently or Strings. Perhaps all publicly available methods in FileDistributionService should be passed Strings of filenames, and in each method which takes a String, a getDirectory().resolve(filename) is done silently, so that the ChunkSever doesn't have to know anything about how the FileDistributionService has created a directory for itself to store files into?

- May be a duplicate entry, but need to think about when sending acknoledgements/responses is actually necessary. The requestsChunk/Shard/Slices all send out denials for the request if it can't be fulfilled. For the requestsChunk/Shard, the requester could be the Client. In that case, maybe a denial is necessary? But the requestSlices message will only come from another ChunkServer, and do we really want communication between ChunkServers to be back-and-forth like that. If we do, we shouldn't block, but should send the first message, and then upon a receiving a response, take another action. But what if there is no response? Based on that, ChunkServers should probably block for responses from other ChunkServers for a certain amount of time, and then close the connection, and move onto the next server that could help to repair a file. Perhaps.

- Should I be checking if the files on the disk are the right length? I think that reads succeed of files have been appended to, but nothing else, because the file is read only up to the specific length that the hashes and data take up, and nothing more.

- idealState and reportedState do not need to be in the ChunkServerConnectionCache. They should be in their own class with methods specific to them.

- Probably need to handle exceptions in the heartbeat() method, so that one exception doesn't stop the entire heartbeat.

- Perhaps all file reads should only read a maximum number of bytes, say, the number of bytes in a properly formatted chunk. If we imagine that some bad actor appended a chunk/shard file with a 10GB video, and called for a read as a client, that would effectively bring down that ChunkServer, as it would be reading for a very long time.

- A fair bit of cleanup to do in the HeartbeatMonitor still. Still need to cleanup and rework the FileDistributionService. And even after cleaning that up, there's still the ChunkServerConnectionCache, and the DistributedFileCache to think about, both of which are just horrendous, I almost think I should tackle a rework of the way the Controller stores files, as the current setup of the DistributedFileCache is too complicated.

- It would simplify things a lot for the Controller not have to keep track of whether the servers storing a particular file are storing shards or replications of chunks.

- The Controller might receive a corruption message from a ChunkServer for a file that it shouldn't have. If it does, it shouldn't respond, or it should send a message to that ChunkServer telling it to delete the file.
