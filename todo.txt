TODO:

1. When a 'ClientRequestsStoreChunk' is sent to the server, and the server hasn't stored that chunk yet, availableChunkServers doesn't automatically create 3 chunks and add them to the chunkcache for every triplet of servers that it gives out. This means that if another requests to store the same chunk comes immediately after the first, before there is a chance for the recommendations structure to be updated, we could store twice. Therefore, it should modify the data structure and add the three servers that it recommends as three chunks into recommendations. Then, if the message isn't actually sent out, the chunks can be removed. /// DONE

2. Need to send CONTROLLER_REQUESTS_FILE_FORWARD to replication servers that have a replication of a chunk held by a deregistered chunk server. If the server acknowledges the request, add the relevant file to the DistributedFileCache with the replacement chunk server's identifier. Maybe send a heartbeat to the replacement chunk server first in order to check if it is alive. May need to actually daisy chain TCP connections to actually get it done. Best effort may not work if we are trying to keep the recommendations server synchronized with who has actually received the request. /// REWORKED

3. REQUESTS_SHARD and SEND_FILE_FOR_STORAGE in TCPReceiverThread is still unfinished. It relies on some functions that need to be written in the FileDistributionService which will read shard files and check if they are corrupt. /// DONE

4. Create a message called CHUNK_SERVER_REPORTS_FILE_HEALTHY, that will be sent to the controller following a fix to a chunk server's file, which could be a result of a slice request or a file forward. // DONE

5. SEND_FILE_FOR_STORAGE will send messages to the controller about Chunk Servers that it can't connect to. It will also send messages to the controller about chunk servers that don't acknowledge the file that was sent to them. When the Controller receives a message like this, it will remove the chunk from its recommendations DistributedFileCache. It won't try to forward immediately. That will just result in more confusion, because there won't have been enough time to actually get confirmation that chunk servers have the file (they inform the controller through heartbeats). So, part of the heartbeat structure now has to include a section where files without three replications in the recommendations cache will be given a third replication server that isn't the other two, and those messages will be sent out of the heartbeat channel. // REWORKED

6. Finish the logic in the FileDistributionService's run method so that it can handle messages of corruption and forwarding as they come through. /// DONE/REWORKED

7. Finish the 'deregister' function in the ChunkServerConnectionCache. New homes must be found for files that the deregistering node is taking down with it. /// DONE

--never-wait-on-replies--
No thread that is responsible for dealing with received messages should send a message over the network AND block for a reply, even if there is a timeout on the blocking. This happens occasionally in the TCPReceiverThread (which itself needs to be rethought), and in the Client. The first rule of a distributed system is that you cannot tell the difference between a dead node and a slow node. Therefore, the message logic should be set up so that if a message is received, it is dealt with in a specific way, and a response is sent back out (if necessary). If sending out a response elicits another message, then THAT message will be dealt with on the next loop of the receiver thread, not the current loop. In this way, nodes can be slow or dead, and we can deal with them in other ways. // DONE

--read-write-in-blocks--
At the Client, files are read one chunk at a time, and sent to their respective ChunkServers for storage. This is inefficient (obviously). Instead, the Client should read in blocks of chunks -- twenty at a time? -- and then work through chunkizing/sharding that data and sending it off to ChunkServers. Likewise should be done when the Client is retrieving a file from the ChunkServers -- multiple chunks should be gotten and assembled in memory, and then written to disk, instead of writing to disk one chunk at a time. // DECIDED AGAINST

--better-control-flow-nodes--
The Controller, ChunkServer, and Client main methods are a mess right now -- especially the Client -- and are in need of a re-write for readability and maintainability. To bring them more up to speed with the principles I've been using for the pastry network. // DONE

--de-monolith-tcpreceiverthread--
The TCPReceiverThread is a monster, but fret not! There are good alternatives to how it currently works! The Client should not have access to methods for receiving messages that are only to be used by the Controller, or the ChunkServer. Same for the other two node types. Changing how this works requires moving to the EventFactory model, which means adding a couple of classes to the project (which have already been written for the pastry project), and then slowly working your way through the TCPReceiverThread -- moving functions for dealing with specific message types into main node classes. Doing so will also create an opportunity to fix most of the occurences of the --never-wait-on-replies-- todo item. // DONE

--event-factory--
EventFactory isn't formatted how it should be. Need to go through message types, and consolidate certain types of messages into more generic categories for simplicity. Having a different message type/format for every single type of message that can be sent is superfluous and confusing. // DONE


LOG:

--tmux--
Created a TMUX script which, instead of spawning new terminal windows and tabs to house the running ChunkServers and the Client, does the same thing using TMUX windows. I was hoping it could be a cleaner solution to running the app locally, perhaps less vulnerable to bugs which could occur in the timing of the creation of those new windows and tabs.

--fat-jar--
The 'build.gradle' file was giving me trouble. I didn't understand how it worked, and still don't fully, but I simplified it while maintaining its functionality. The tricky part is forcing gradle to bundle the 'reed-solomon-erasure-coding.jar' with the project jar at the end so that the classes like FileDistributionService can find the ReedSolomon functions during execution.

--docker--
Created a new folder in the upmost project directory called 'docker', which contains both a 'Dockerfile' and a 'docker-compose' file. Having just taken a course covering the basics of docker, I wanted to use what I had learned to provide a way of running this app in an isolated environment. It's a complete building and running solution which uses a Gradle container to compile, and a Java container to run the necessary components. Each node type -- controller, chunkserver, and client -- has a slightly different container setup which is specified in the dockerfile. A Gradle container compiles the project once, and a copy of the compiled JAR file is given to each of the node containers to execute. Once started, the containers communicate with one another via a docker virtual network (created in the compose file), which allows containers to route to eachother via their container names. To run the app with docker, the user must first change the 'controllerHost' variable inside the 'application.properties' file to 'controller'. Then, from inside the 'docker' folder, the user must run 'docker compose build' in the command line, followed by 'docker compose up -d'. The first command builds the project, and the second command creates and starts container nodes in the background. Then, the user can use 'docker attach dfs-client-1' to attach to the running Client's I/O stream, so that they may run commands like 'put', 'get', 'ls', etc. To exit out of the I/O stream the user can use the docker escape sequence: Ctrl-p Ctrl-q. 'docker compose down' will stop all of the containers and remove them.

--properties--
After adding a docker option for running the app, it was clear that it would be too complicated to ask the user to change the values of variables within the Controller, ChunkServer, and Client classes to reflect the hostname and port where the Controller could be found. I had recently been working on another distributed systems project for the same course, and had borrowed a solution which uses Java Properties to read a file for variables that the user should be able to change. So, I copied two classes I'd completed in that project into this project, and created another folder in the upmost project directory called 'config', which contains a file called 'application.properties'. This file contains values for variables that the user can change, specifically the ones that need to be changed when running the app locally, in docker, or in a distributed environment. With this new file, the user no longer passes either 'erasure' or 'replication' to the Client as a command line argument, but chagnes the value in the 'application.properties' file.

--osx-tab--
The 'osx.sh' script was producing strange behavior. Newly created terminals/tabs default their working directory to the home directory (~/), and if a command is executed too quickly, the tab has no chance to change it to the directory of the parent terminal. Therefore, the newly created ChunkServers couldn't find the 'application.properties' file, which is found by concatenating the path from which the JAR was started with '/config/application.properties'. Worked around this issue by adding a delay between spawning the new terminal tab and executing the start command. // PROBLEM WAS TOO MANY SLOW SCRIPTS IN BASHRC

--refactor-nodes--
Working on refactoring the code at the Client and the Controller to be more readable, and to be more in the style of the code of the nodes in the pastry network assignment. Also have been fixing some formatting issues throughout the project -- function and variable names that go against the camelCase style that I've been using dominantly.

--brainstorming-client--
The Client doesn't need to to make use of the TCPReceiverThread that must be modified for use by Controller and the ChunkServer, so what is the best way for the Controller to communicate with the other nodes? The Client has a main thread which receives commands from the user, and although in a perfect world it would be better to keep the main thread free to take commands from the user, even while a write or a read to or from the dfs is happening, to start, I'll just write a ClientWriter and ClientReader which will be Runnable, and can perform those jobs while the main thread waits, or checks for progress on the job.



- Cleaned up eventfactory, made it a singleton

- Cleaned up TCPSender

- Starting to transfer TCPReceiverThread cases into appropriate node

- DistributedFileCache is a mess, need to improve underlying data structures of ServerFile, DistributedFile, Shard, Chunk to really come up with something easier to maintain and understand. There are too many function calls associated with getting just one piece of information, too much conversion between String, Integer, Long for there to be any comprehension of what is going on. // FIXED

- Need better solution from Comparators in DistributedFileCache // FIXED

- Deregistration can't be one-off best effort, must be able to keep track of state of relocations and try again if relocation fails until options are exhausted. // FIXED

- Need to go through all the messages and make getType() actually return the type stored in the message, not the predefined protocol. Also need to go through the messages and make them match GeneralMessage in their formatting for readability and consistency. // DONE

- Make sure all messages that can are using the GeneralMessage instead of a superfluous specific  message type. How does changing to use the GeneralMessage at the EventFactory and the Controller affect the logic at the ChunkServers and the Client? Need to think about that. // DONE

- BlockingQueue for identifiers or ConcurrentMap for ChunkServerConnectionCache? It seems like it could improve the readability of the code, with less manual locking. // DECIDED AGAINST

- Add timestamp and version information to the Chunks and Shards. Both should be easy. Timestamp information and Version count will each be updated any time a write happens to a Chunk or a Shard. If the shard is replaced at the ChunkServer, its timestamp will be updated to the time of its writing, and its version will be incremented from wherever the Shard's data came from. Whereever the shard came from, its version will be +1 of what it was before. But, do chunks that are identical except the version number should probably be treated identically, same with the shards. // DONE

- Why is listFreestServers() returning a Vector<String> and not a Vector<Integer>? Shouldn't it be returning a list of server identifiers, which are integers? // IRRELEVANT NOW

- Instead of returning null values, some functions choose to return -1 or "" (for int and string), but why? Is there a reason that null can't convey the same thing? There seem to be different standards used throughout the code. // CAN'T RETURN null IF RETURNS int

- What about the scenario where the Client requests to store a chunk, the Controller finds three ChunkServers that should hold that particular chunk, but one of those ChunkServers goes offline or deregisters before the Client has had a chance to store the chunk at that particular ChunkServer? How will the Client know if a storage operation is successful? Should the Client send the chunk out to the ChunkServers without wondering whether the operation was successful? Should it wait for confirmation from the ChunkServers? If it does wait for confirmation, but doesn't receive it, what should happen then? It seems like the Client should give the user some indication that the storage operation only successfully happened at N number of nodes. But if it is done that way, how long should the Client wait for confirmation before returning control of the Client to the user? Best-effort and conscientious communication with the user are at odds with one another. // CLIENT DOESN'T COUNT, SENDS OUT BLINDLY

- When deregistering, we shouldn't do any unnecessary deletions of the data structures that are being removed from the ChunkServerConnectionCache. That way, we won't disrupt any threads that are doing inside of the ChunkServerConnection while it is being deregistered. // IRRELEVANT NOW

- Need to create a ChunkInformation class just for storing metadata about a particular chunk/shard. Name, sequence, version, timestamp, etc. Then, that data structure can be used to replace the delimited strings that are being passed around in the ChunkServerHeartbeatService and the HeartbeatMonitor. // CALLED FILEMETADATA

- More to add to deal with CHUNK_SERVER_NO_STORE_FILE case at the Controller. Need to find a new home for the missing Chunk, or is that dealt with elsewhere? // NOT USING ANYMORE

- Should there even be a Shard class? Since the shard is basically just a particular replication of a chunk under the erasure coding storageType? // NO, GOT RID OF IT

- Need to go through wireformats and delete message types that are now just using the GeneralMessage for the same purpose. This will create errors, which will lead to locations where the GeneralMessage can be instantiated instead of the other, deleted message types. // DONE

- Have thought about this before, but should the Controller just send the Client the list of ChunkServers from the idealState or the reportedState. The problem with the reportedState is that it isn't available right away, there is a delay until the reportedState starts being populated by heartbeats. But, it does represent the actual state of the DFS assuming heartbeats are accurate. What is the best option here? // NO LONGER USING IDEAL/REPORTED STATE

- There must be an elegant way to start the TCPServerThread and ChunkServerHeartbeatService and the FileDistributionService only once the Controller has sent confirmation to the ChunkServer of a successful registration? // TRIED MY BEST

- May need to come up with a better way of generating the folder that the ChunkServer will use to store files. Would it be better to create unique filenames (with host and port and id attached) than to create a folder with a unique timestamp? How could this be made cleaner, instead of being done in the registrationInterpreter? // STILL DON'T USE FANCY FILENAMES. LESS OF A PROBLEM NOW WITH THE FILETABLE

- Maybe can start the FileDistributionService before the registration happens in the ChunkServer? It would be inactive if the ChunkServer isn't registered anyway. // NOT USING FILEDISTRIBUTIONSERVICE ANYMORE

- Should open connections between nodes be periodically cleared? I imagine it would become inefficient if every ChunkServer maintained an open connection to every other ChunkServer, as would happen after the DFS has been up for a while, what with file repairs and forwarding. // TODO

- Get rid of sendQueue in ChunkServerConnection, better for the Controller to be made aware of any error being thrown as it tries to send a message to a ChunkServer. // DONE

- sendGeneralMessage, sendRegistration, and sendDeregistration should all behave similarly, in that they should all either throw an exception, or shouldn't, which is best for the ChunkServer? And, should they all be added to a different class which houses functions for sending messages out of specified connections? Actually, it will be best to get rid of the sendRegistration and sendDeregistration functions, and just use the sendGeneralMessage with the correct types and messages for registration and deregistration. sendGeneralMessage should throw an exception if it fails. // NOT USING sendRegistration/Deregistration

- getDirectory() in FileDistributionService has changed, and now returns a Path, not a string. Therefore, any place this function has called must now be inspected to ensure that the output of getDirectory() is not concatted with a String, as was the behavior before. Instead of concatenation, new usage will be to use getDirectory().resolve(filename) to create a path object that points to the file we're trying to operate on. // DONE

- The "Files" package does support appending files, if you use additional options in the call to Files.write(), but it does not support editing a section of a file in place. That shouldn't be an issue in this scenario, as chunks are 64K and shards are around 8K -- the file can be read fully, and the byte string can be edited, and then the byte string can be rewritten -- but this will have to be implemented in the replaceSlices(), truncateFile()?, appendFile(), and getNextChunkFromFile()? // WANTED TO USE Files CLASS, BUT CAN'T IN SOME INSTANCES

- Decide whether to work with Paths consistently or Strings. Perhaps all publicly available methods in FileDistributionService should be passed Strings of filenames, and in each method which takes a String, a getDirectory().resolve(filename) is done silently, so that the ChunkSever doesn't have to know anything about how the FileDistributionService has created a directory for itself to store files into? // DONE

- May be a duplicate entry, but need to think about when sending acknoledgements/responses is actually necessary. The requestsChunk/Shard/Slices all send out denials for the request if it can't be fulfilled. For the requestsChunk/Shard, the requester could be the Client. In that case, maybe a denial is necessary? But the requestSlices message will only come from another ChunkServer, and do we really want communication between ChunkServers to be back-and-forth like that. If we do, we shouldn't block, but should send the first message, and then upon a receiving a response, take another action. But what if there is no response? Based on that, ChunkServers should probably block for responses from other ChunkServers for a certain amount of time, and then close the connection, and move onto the next server that could help to repair a file. Perhaps. // NOT SENDING MANY ACKNOWLEDGEMENTS. NO LONGER USING FILEDISTRIBUTIONSERVICE FOR FILE REPAIRS

- Should I be checking if the files on the disk are the right length? I think that reads succeed of files have been appended to, but nothing else, because the file is read only up to the specific length that the hashes and data take up, and nothing more. // ONLY READING WHAT THE LENGTH SHOULD BE

- idealState and reportedState do not need to be in the ChunkServerConnectionCache. They should be in their own class with methods specific to them. // NOT EVEN USING THEM NOW

- Probably need to handle exceptions in the heartbeat() method, so that one exception doesn't stop the entire heartbeat. // DONE

- Perhaps all file reads should only read a maximum number of bytes, say, the number of bytes in a properly formatted chunk. If we imagine that some bad actor appended a chunk/shard file with a 10GB video, and called for a read as a client, that would effectively bring down that ChunkServer, as it would be reading for a very long time. // DONE

- A fair bit of cleanup to do in the HeartbeatMonitor still. Still need to cleanup and rework the FileDistributionService. And even after cleaning that up, there's still the ChunkServerConnectionCache, and the DistributedFileCache to think about, both of which are just horrendous, I almost think I should tackle a rework of the way the Controller stores files, as the current setup of the DistributedFileCache is too complicated. // DONE

- It would simplify things a lot for the Controller not have to keep track of whether the servers storing a particular file are storing shards or replications of chunks. // FIXED

- The Controller might receive a corruption message from a ChunkServer for a file that it shouldn't have. If it does, it shouldn't respond, or it should send a message to that ChunkServer telling it to delete the file. // NOT POSSIBLE NOW, FILETABLE STOPS THAT

- Need to gracefully shutdown the ChunkServer after sending a deregistration request. This will include closing all open TCPConnections and cancelling the heartbeat Timer. // DONE (HOPEFULLY)

- It is possible to simplify the RepairFile logic at the ChunkServer. Since the ChunkWriter takes a byte[][] of slices paired with a int[] of indices, that could be simplified to just take a byte[8][] of slices which is always length eight, but with null values for slices that weren't retrieved. That would align with the ShardWriter's getReconstructionShards() method, which takes a byte[9][] array, with null values for fragments not retrieved. // TODO MAYBE

- There is a small chance that when a ChunkServer receives a RepairChunk or RepairShard message, and it isn't the destination, a read of its local file reveals corruption. If that happens, should it report its own corruption to the Controller? Should it try to use the RepairMessage to repair its own corruption first, and then contact the Controller? What should it do? // TODO MAYBE

- listFiles() in FileDistributionService feels so confusing for what it does. Is using Files really the best option? Should we return a List<> instead of a String[]? // IS CONFUSING BUT STILL USING IT

- Instead of choosing which type of FileReader to instantiate based on the filename, could use the application.properties value for the storage schema type instead? Right now, the ChunkServers are set up to be able to store shards and chunks at the same time, and so is the Controller -- it will allocate servers for either chunks or shards, but what type of storage the Client asks the Controller for will be determined by the value in the application.properties file. Right now, it just feels like the Controller and ChunkServer are overbuilt given that the Controller cannot be given a command line argument for how it stores files. If you could start a Client to use erasure coding, shut it down (without shutting down all the nodes, and start it up again using replication, the current design would make sense, but if the Client will always use the storage schema in application.properies, the Controller and Chunkserver should be changed. // CLIENT CAN NO LONGER BE STARTED THIS WAY, DON'T HAVE TO WORRY ABOUT IT

- TCPConnectionCache needs to have some way to clear inactive connections from the cache. The getConnection() method should never return a stale connection. And there should be a method to clear the connections. // TODO MAYBE (ON THE STALE CONNECTION PROBLEM)

- Need to make a choice about whether to use the full path as a filename to store on the ChunkServers or use just the filename. // JUST FILENAME

- Need to make a note that in the ChunkServerConnectionCache, I changed getChunkStorageInfo() to get its information from the idealState, not the reportedState. The idealState will not have corruption. // NOT USING EITHER ANYMORE

- When the Client is reading a file, it should get information from the Controller about where to get chunks one chunk at a time. That way, responses from the Controller can only be invalidated for that particular chunk, not the entire file (if say, a server deregisters right after the Client requests information about where a chunk is stored, and that chunk happens to be stored at the server that is in the process of deregistering). // DECIDED AGAINST, NOW RETURN LOCATIONS FOR ENTIRE FILE

- The HeartbeatMonitor poses a problem for the Controller. It runs regardless what other events are being processed. But, since the events being processed are going to need synchronization, the HeartbeatMonitor must be able to ensure that no Server registers or deregisters during its run, and no files are allocated. This means that the HeartbeatMonitor needs to use a synchronized(Controller) lock, or, if not, need to figure out a different way to stop events being processed from interfering with the HeartbeatMonitor. // USING FULL SYNCHRONIZATION

- FileDistributionService should be renamed to 'FileSynchronizer' or something like that. // DONE

- What if a two messages come into the Controller in succession -- the first is a message about a chunk/shard being corrupt, and the next is a deregistration request. The Controller needs to be aware of the list of files the deregistering ChunkServer is supposed to be holding, but the deregistering ChunkServer's address will already have been removed from the array in at a particular filename and sequence number. So, how does the Controller know that it needs to find a new home for the file that is corrupt? Hopefully it will work that when the Controller gets a notification of corruption, the host:port is removed from the table, but not from the HashTable<String,<ArrayList<Integer>> inside the ServerConnection. That way, when the ChunkServer deregisters, the proper entry will still be in that hashtable, and when trying to replace all host:port instances in the real table we can use the secondary hashmap to look for the entry in the large table, and find a replacement ChunkServer then. The only problem might be that if we are erasure coding, and a ChunkServer deregisters with a file that has been marked corrupt, we'll have no way of knowing which fragment that ChunkServer was storing -- we'll only have the base filename and sequence number. And, if the array holding server host:ports in the table has more than one null entry, we won't be able to figure out which null entry is associated with the deregistering ChunkServer. So, either we need to change the secondary table to accomodate a fragment number, or we need to replace what was supposed to be a host:port in the server array with something else, indicating that the thing is corrupt -- perhaps instead of replacing it with null, we could replace it with the string of the identifier of the ChunkServer who is dealing with the corruption. But if we do that, we'll have to deal with other edge cases where there are string identifiers in the table where there should be addresses, and if we try to get a connection based on an identifier, we'll throw an error. // DECIDED NOT TO REPLACE CORRUPT CHUNKS WITH NULL IN THE FILETABLE

- The Client can still send either a CLIENT_STORE_CHUNK or CLIENT_STORE_SHARD message, but the Controller is now only operating based on what is in the 'application.properties' file, so the Client should be doing that too. That means one message type to store a Chunk. // FIXED

- Still need to fix the HeartbeatMonitor to replace files that aren't currently stored at the ChunkServer. Will just do this by comparing the file list at every MajorHeartbeat with the list of chunks in the corresponding ServerConnection. Any chunks in the ServerConnection that aren't in the heartbeat should be replaced. But, this will only truly make sense when the ChunkServers are changed so that they don't read all theire files every heartbeat, but instead keep a list of them in memory, and only read them when the reads are requested. // DONE, BUT NOT EXACTLY IN THIS WAY

- Building on the previous bullet, the ChunkServers need to be changed so that they don't read all their files every heartbeat, but instead only read their files when asked -- and so only report corruption then, not during heartbeats. // DONE

- The relocate process upon deregistration doesn't seem to be working for erasuring coding. It is working for replication, but the file stored at the replacement server keeps complaining that it is corrupt, even though the file is the correct length. After adding a print statement, it believes that all 8 slices are corrupt, every time. Something must be going wrong with replacing the hashes and writing the file. // FIXED

- When a ChunkServer registers, and registers successfully, there should be a routine that runs at the Controller to check to see if there are any chunks that don't have a proper replication factor, and to task the new registrant with storing that replica/shard. // TODO

- The correct slices are getting to the destination server, but they'r e not getting copied correctly to the slices array before getting written to the disk. // FIXED

- There is an issue with synchronization of the ChunkServer. The HeartbeatService gets a list of files in that it is storing, but if a delete request comes in after the list has been created but before all the files have been dealt with, the HeartbeatService interprets a deleted file as being corrupt, and sends a corruption message to the Controller. This can be solved with synchronization, much like how it was solved in the Controller, by forcing the HeartbeatService to lock the ChunkServer's intrinsic lock. For this to work, the ChunkServer's methods will need to be examined, and the 'synchronized' keyword will have to be added to some of them. // FIXED

- Files stored by the ClientReader seem to be missing some bytes at the end. Is are the chunks getting cutoff at the very end by the getContentFromShards() function? // FIXED

- Don't forget: This should be basic, but sending sockets must give receiving sockets time to read what was sent before shutting down. The ClientWriter was shutting down directly after all chunks had been sent out, preventing receiving sockets at the servers from reading those chunks before the socket was closed. // FIXED, ADDED SLEEP

- ChunkServers should try to reinitialize their connection to the Controller if it becomes broken. It'll have to be done with synchronization, as the controllerConnection might be accessed by mulitple threads reading corrupt files, or the HeartbeatService. // TODO MAYBE, PROBABLY NOT

- During slow downloads to the Client, the ChunkServers appear to be blocked, preventing Heartbeats from being sent. How can this be solved? //DOESN'T SEEM TO HAPPEN ANYMORE

- To make the ChunkServers keep track of the FileMetadata's they should be storing, all storage, deletion, and repair operations will need to be synchronized on the data structure storing the file metadata. Otherwise, the FileMetadata could be changed by thread one, then changed by thread two, then written to disk by thread one. That sort of interleaving would not leave the data structure consistent with what is stored on disk, which is undesirable. Here are the instructions for how to use the FileMetadata structure (probably a map) in its different use-cases:
    - SendsFileForStorage:
        - Check if there is a file with that name already
        - If there is update the version and timestamp, if there isn't create a new entry
        - Write the file to disk
        - Even if the write fails, don't remove the FileMetadata just added to the map
    - RepairShard:
        - If you are the destination, check to see if you have metadata for the file
        - If you do, update the version number and timestamp, and set the shardreader's FileMetadata to the one you just updated
        - If you don't create a new FileMetadata for the shard, add it to the map, and then set the shardreader's FileMetadata to it
        - The ShardWriter will have to be modified to always use the FileMetadata in the shardreader for writing a shard to disk
        - Write the shard to disk using the updated FileMetadata
    - RepairChunk:
        - Same as the steps for RepairShard
    - DeleteRequest:
        - Synchronize on the map of file metadatas
        - Delete all file metadatas that have that particular basename
        - Then try to delete from disk all files that have that basename
    - HeartbeatService:
        - The service must now synchronize on the FileMetadata map so that no changes are made while the heartbeat is assembled
// DONE

- Implement a proper logger with correct log levels. Then add another property in the application.properties file to specify the logging level. // DONE

- As it is currently programmed, the Controller never figures out that a chunk is no longer able to be repaired. // TODO MAYBE

- The reason for creating a custom Logger would be so that you could set the log level in the application.properties file without having to also use a logging.properties file, as java.util.logging doesn't support setting the global log level programmatically, only through a config file.

- The dispatchRepair method of the HeartbeatMonitor works well in principle, but because the HeartbeatMonitor doesn't know when certain files were allocated, it can't refrain from trying to replace missing files that are new and haven't been sent by the Controller yet. Also, the 'stop' command at the Controller is flawed in that it can cause deadlock. // FIXED

- What might have happened was that between calls to the Controller by the Client to allocateServers, the ChunkServers sent major heartbeats. The HeartbeatMonitor acquired the intrinsic lock for the Controller, and proceeded with its functions. The Client waited for a return message.The HeartbeatMonitor detected files that weren't stored by the proper ChunkServers (because they were still in the process of being delivered to them by the Client), and tried to dispatch repairs for those files, for all ChunkServers that were sending major heartbeats (which all of them were). When I tried to use the 'stop' command for the writer, the writer wasn't waiting, so the stop() function blocked indefinitely until it could acquire the intrinsic lock of the writer, which never became available. I wish I could know where the ClientWriter itself was blocking. // DON'T KNOW BUT THINK IT'S FIXED

- Randomize the order of the servers given to the Client when they ask for the severs storing a particular chunk, but only if using replicationand not erasure coding. Hopefully this better distributed the load of requests, and increases the liklihood of detecting corruption before it becomes a problem. // TODO MAYBE

- Make the HeartbeatInformation keep track of successive major heartbeats to solve the problem of the Controller prematurely trying to replace missing files at ChunkServers. // FOUND ANOTHER SOLUTION

- Instead of detecting missing files at the Controller, wouldn't it be easier to, every major heartbeat, check them at the ChunkServer -- perform an ls on the folder, and report corruptions for every file that is in the fileTable but not in the folder?
ANSWER: Yes, that could be done, but it takes the responsibility for maintaining a proper replication factor out of the hands of the Controller. It would be a minor violation of division of responsibility.

- Why not use Circle Plots to visualize the data patterns for the Controller, Client, and ChunkServer during operation? // TODO MAYBE
